{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cats_dir = '../input/cat-and-dog/training_set/training_set/cats/'  # directory with our training cat pictures\ntrain_dogs_dir = '../input/cat-and-dog/training_set/training_set/dogs/'  # directory with our training dog pictures\nvalidation_cats_dir = '../input/cat-and-dog/test_set/test_set/cats/' # directory with our validation cat pictures\nvalidation_dogs_dir = '../input/cat-and-dog/test_set/test_set/dogs/'  # directory with our validation dog pictures\ntrain_dir = '../input/cat-and-dog/training_set/training_set/'\nvalidation_dir = '../input/cat-and-dog/test_set/test_set/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cats_tr = len(os.listdir(train_cats_dir))\nnum_dogs_tr = len(os.listdir(train_dogs_dir))\n\nnum_cats_val = len(os.listdir(validation_cats_dir))\nnum_dogs_val = len(os.listdir(validation_dogs_dir))\n\ntotal_train = num_cats_tr + num_dogs_tr\ntotal_val = num_cats_val + num_dogs_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('total training cat images:', num_cats_tr)\nprint('total training dog images:', num_dogs_tr)\n\nprint('total validation cat images:', num_cats_val)\nprint('total validation dog images:', num_dogs_val)\nprint(\"--\")\nprint(\"Total training images:\", total_train)\nprint(\"Total validation images:\", total_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 128\nepochs = 15\nIMG_HEIGHT = 150\nIMG_WIDTH = 150","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our training data\n#remember the rescale is because we want not values between 0-255, but between 0-1 !!\nvalidation_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our validation data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n                                                           directory=train_dir,\n                                                           shuffle=True,\n                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n                                                           class_mode='binary')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,\n                                                              directory=validation_dir,\n                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),\n                                                              class_mode='binary')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_training_images, _ = next(train_data_gen)\n#what is the _ for ?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This function will plot images in the form of a grid with 1 row and 5 columns where images are placed in each column.\ndef plotImages(images_arr):\n    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n    axes = axes.flatten()\n    for img, ax in zip( images_arr, axes):\n        ax.imshow(img)\n        ax.axis('off')\n    plt.tight_layout()\n    plt.show()\n    #what's this zip bit?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotImages(sample_training_images[:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential([\n    Conv2D(16, 3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),\n    MaxPooling2D(),\n    Conv2D(32, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Conv2D(64, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Flatten(),\n    Dense(512, activation='relu'),\n    Dense(1) #why is it 1 and not 2 since 2 classes?\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), #binary since 2 classes\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit_generator(\n    train_data_gen,\n    steps_per_epoch=total_train // batch_size,\n    epochs=epochs,\n    validation_data=val_data_gen,\n    validation_steps=total_val // batch_size\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss=history.history['loss']\nval_loss=history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Augmentation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"image_gen = ImageDataGenerator(rescale=1./255, horizontal_flip=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#here we are adding horizontal flipping\ntrain_data_gen = image_gen.flow_from_directory(batch_size=batch_size,\n                                               directory=train_dir,\n                                               shuffle=True,\n                                               target_size=(IMG_HEIGHT, IMG_WIDTH))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"augmented_images = [train_data_gen[0][0][0] for i in range(5)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Re-use the same custom plotting function defined and used\n# above to visualize the training images\nplotImages(augmented_images)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#here we are trying randomly giving 45degree rotation\nimage_gen = ImageDataGenerator(rescale=1./255, rotation_range=45)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_gen = image_gen.flow_from_directory(batch_size=batch_size,\n                                               directory=train_dir,\n                                               shuffle=True,\n                                               target_size=(IMG_HEIGHT, IMG_WIDTH))\n\naugmented_images = [train_data_gen[0][0][0] for i in range(5)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"plotImages(augmented_images)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now for random zooms of 50% on images\n# zoom_range from 0 - 1 where 1 = 100%.\nimage_gen = ImageDataGenerator(rescale=1./255, zoom_range=0.5) # ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_gen = image_gen.flow_from_directory(batch_size=batch_size,\n                                               directory=train_dir,\n                                               shuffle=True,\n                                               target_size=(IMG_HEIGHT, IMG_WIDTH))\n\naugmented_images = [train_data_gen[0][0][0] for i in range(5)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotImages(augmented_images)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now, apply the augmentations and re-train the CNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"image_gen_train = ImageDataGenerator(\n                    rescale=1./255,\n                    rotation_range=45,\n                    width_shift_range=.15,\n                    height_shift_range=.15,\n                    horizontal_flip=True,\n                    zoom_range=0.5\n                    )\n#all at once now!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_gen = image_gen_train.flow_from_directory(batch_size=batch_size,\n                                                     directory=train_dir,\n                                                     shuffle=True,\n                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n                                                     class_mode='binary')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets look at augmented image!\naugmented_images = [train_data_gen[0][0][0] for i in range(5)]\nplotImages(augmented_images)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#make validation data again, but this is NOT augmented!\nimage_gen_val = ImageDataGenerator(rescale=1./255)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_data_gen = image_gen_val.flow_from_directory(batch_size=batch_size,\n                                                 directory=validation_dir,\n                                                 target_size=(IMG_HEIGHT, IMG_WIDTH),\n                                                 class_mode='binary')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit_generator(\n    train_data_gen,\n    steps_per_epoch=total_train // batch_size,\n    epochs=epochs,\n    validation_data=val_data_gen,\n    validation_steps=total_val // batch_size\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualize the new model, hopefully overfitting isn't as bad?\n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dropouts to avoid overfitting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#model_new = Sequential([\n#    Conv2D(16, 3, padding='same', activation='relu', \n#           input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),\n#    MaxPooling2D(),\n#    Dropout(0.2),\n#    Conv2D(32, 3, padding='same', activation='relu'),\n#    MaxPooling2D(),\n#    Conv2D(64, 3, padding='same', activation='relu'),\n#    MaxPooling2D(),\n#    Dropout(0.2),\n#    Flatten(),\n#    Dense(512, activation='relu'),\n#    Dense(1)\n#])\n\n#we are adding dropouts after the first and final maxpooling layers. Applying dropout will randomly set 20% of the neurons to zero during each training epoch. This helps to avoid overfitting on the training dataset.\n#When you apply dropout to a layer it randomly drops out (set to zero) number of output units from the applied layer during the training process. \n#Dropout takes a fractional number as its input value, in the form such as 0.1, 0.2, 0.4, etc. This means dropping out 10%, 20% or 40% of the output units randomly from the applied layer.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model_new.compile(optimizer='adam',\n#                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n#                  metrics=['accuracy'])\n\n#model_new.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#training again, now with dropouts added and image augmentation done\n#history = model_new.fit_generator(\n#    train_data_gen,\n#    steps_per_epoch=total_train // batch_size,\n#    epochs=epochs,\n#    validation_data=val_data_gen,\n#    validation_steps=total_val // batch_size\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualize the new model, hopefully overfitting isn't as bad?\n#val accuracy is worse with the drop out.\n\n#acc = history.history['accuracy']\n#val_acc = history.history['val_accuracy']##\n#\n#loss = history.history['loss']\n#val_loss = history.history['val_loss']\n#\n#epochs_range = range(epochs)\n#\n#plt.figure(figsize=(8, 8))\n#plt.subplot(1, 2, 1)\n#plt.plot(epochs_range, acc, label='Training Accuracy')\n#plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n#plt.legend(loc='lower right')\n#plt.title('Training and Validation Accuracy')\n#\n#plt.subplot(1, 2, 2)\n#plt.plot(epochs_range, loss, label='Training Loss')\n#plt.plot(epochs_range, val_loss, label='Validation Loss')\n#plt.legend(loc='upper right')\n#plt.title('Training and Validation Loss')\n#plt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}